{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file1_path,file2_path):\n",
    "    dataset1_reader = csv.reader(open(file1_path, encoding='utf-8'))\n",
    "    dataset2_reader = csv.reader(open(file2_path, encoding='utf-8'))\n",
    "    # define a list to store the data\n",
    "    all_data_set = []\n",
    "\n",
    "    # read the data into a list(name,sequence,class)\n",
    "    for row in dataset1_reader:\n",
    "        all_data_set.append([row[0],row[1],row[2]])\n",
    "    for row in dataset2_reader:\n",
    "        all_data_set.append([row[0],row[1],row[2]])    \n",
    "    # shuffle the data set randomly    \n",
    "    random.seed(2)\n",
    "    random.shuffle(all_data_set)\n",
    "    return all_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(dataset):\n",
    "    # get the maxmium length of the seqence\n",
    "    max_seq_len = 0\n",
    "    for item in dataset:\n",
    "        if len(item[1])>max_seq_len:\n",
    "            max_seq_len = len(item[1])\n",
    "    #print(max_seq_len)\n",
    "    #max_seq_len = 169\n",
    "    # padding with \"N\" to max_seq_len\n",
    "    for item in dataset:\n",
    "        item[1] += \"N\" *(max_seq_len-len(item[1]))\n",
    " \n",
    "    # tranformation of data set:one_hot encoding\n",
    "    x_cast = {\"A\":[[1],[0],[0],[0]],\"U\":[[0],[1],[0],[0]],\\\n",
    "              \"T\":[[0],[1],[0],[0]],\"G\":[[0],[0],[1],[0]],\\\n",
    "              \"C\":[[0],[0],[0],[1]],\"N\":[[0],[0],[0],[0]]}\n",
    "    y_cast = {\"TRUE\": [1,0],\"FALSE\":[0,1]} #TRUE:Mirtrons  FALSE:canonical microRN\n",
    "    \n",
    "    # define a list to store the vectorized data\n",
    "    \n",
    "    x=[]\n",
    "    y=[]\n",
    "    for item in dataset:\n",
    "         data = []\n",
    "         for char in item[1]:\n",
    "             data.append(x_cast[char])\n",
    "         #data= np.asarray(data)\n",
    "    \n",
    "         x.append(data)\n",
    "         y.append(y_cast[item[2]])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(786, 164, 4, 1)\n",
      "(786, 2)\n",
      "(338, 164, 4, 1)\n",
      "(338, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "FILE_PATH = \"/home/emon/Downloads/cnnMirtronPred-master/data/miRBase_set.csv\"\n",
    "FILE_PATH_PUTATIVE = \"/home/emon/Downloads/cnnMirtronPred-master/data/putative_mirtrons_set.csv\"\n",
    "\n",
    "# read datasets and merge as a numpy array\n",
    "all_data_array = read_data(FILE_PATH,FILE_PATH_PUTATIVE)\n",
    "\n",
    "# vectorization of the dataset\n",
    "x,y = vectorize_data(all_data_array)\n",
    "x= np.array(x)\n",
    "y=np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.30, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#hyperparameters\n",
    "LR = 0.0001       #learning rate\n",
    "TRAINING_ITER = 10000   #iteration times\n",
    "BATCH_SIZE = 18        #batch size of input\n",
    "\n",
    "SEQUENCE_LENGTH = 164   #sequence length of input\n",
    "EMBEDDING_SIZE = 4      #char embedding size(sequence width of input)\n",
    "\n",
    "CONV_SIZE = 3    #first filter size\n",
    "CONV_DEEP = 128   #number of first filter(convolution deepth)\n",
    "\n",
    "STRIDES = [1,1,1,1]  #the strid in each of four dimensions during convolution\n",
    "KSIZE = [1,164,1,1]    #pooling window size\n",
    "\n",
    "FC_SIZE = 1024     #nodes of full-connection layer\n",
    "NUM_CLASSES = 2   # classification number\n",
    "\n",
    "DROPOUT_KEEP_PROB = 0.4   #keep probability of dropout\n",
    "dataset_size = len(X_train)\n",
    "# define placeholder\n",
    "input_X = tf.placeholder(tf.float32,[None,SEQUENCE_LENGTH,EMBEDDING_SIZE,1])\n",
    "input_y = tf.placeholder(tf.float32,[None, NUM_CLASSES])\n",
    "keep_prob = tf.placeholder(tf.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function of initialize weights\n",
    "def get_weights_variable(shape):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.01)\n",
    "    weights = tf.Variable(initial,name = \"weights\")\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function of convolution and pooling\n",
    "def conv_and_pooling(input_tensor,filter_height,filter_width,\\\n",
    "                     depth,conv_deep,layer_name):\n",
    "    \n",
    "    with tf.name_scope(layer_name):\n",
    "        conv_weights = get_weights_variable\\\n",
    "                    ([filter_height,filter_width,depth,conv_deep])\n",
    "        conv_bias = tf.Variable(tf.constant(0.1,shape=[conv_deep]),name = \"bias\")   \n",
    "        conv = tf.nn.conv2d(input_tensor,conv_weights,strides = STRIDES,\\\n",
    "                            padding='SAME')\n",
    "        #print(np.size(conv))\n",
    "        conv_relu = tf.nn.relu(tf.nn.bias_add(conv,conv_bias))\n",
    "        conv_relu_pool = tf.nn.max_pool(conv_relu,ksize=KSIZE,strides=STRIDES,padding='VALID')\n",
    "        #print(np.size(conv_relu_pool))\n",
    "        # tensorboard visualization\n",
    "        tf.summary.histogram(\"../../data/conv_weights\",conv_weights)\n",
    "        tf.summary.histogram(\"../../data/conv_bias\",conv_bias)\n",
    "        return conv_relu_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_output_inference(input_tensor,fc_size,output_size,keep_prob):\n",
    "    shape_list = input_tensor.get_shape().as_list()\n",
    "    nodes = shape_list[1]*shape_list[2]*shape_list[3]\n",
    "    reshaped = tf.reshape(input_tensor,[-1,nodes])\n",
    "\n",
    "    # the first fully connected layer\n",
    "    fc1_weights = get_weights_variable([nodes,fc_size])\n",
    "    fc1_bias = tf.Variable(tf.constant(0.1,shape=[fc_size]))\n",
    "    fc1 = tf.nn.relu(tf.matmul(reshaped,fc1_weights) + fc1_bias)\n",
    "    \n",
    "    # avoid overfitting, droupout regularization\n",
    "    fc1 = tf.nn.dropout(fc1,keep_prob)\n",
    "    #fc1 = tf.nn.dropout(fc1,0.5)\n",
    " \n",
    "    # the second fully connected layer(output layer)\n",
    "    fc2_weights = get_weights_variable([fc_size,output_size])\n",
    "    fc2_bias = tf.Variable(tf.constant(0.1,shape=[output_size]))\n",
    "    output = tf.nn.relu(tf.matmul(fc1,fc2_weights) + fc2_bias)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only one kinds of filter in the CNN structure\n",
    "def cnn_mono_inference(input_tensor,filter_height,filter_width,\\\n",
    "                  in_channels,out_channels,layer_name,keep_prob):\n",
    "    # layer of convolution and max-pooling\n",
    "    conv_pool = conv_and_pooling(input_tensor,filter_height,\\\n",
    "                                  filter_width,1,out_channels,layer_name)\n",
    " \n",
    "    output = fc_output_inference(conv_pool,FC_SIZE,NUM_CLASSES,keep_prob)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# use different sizes of filters in the CNN structure\n",
    "def cnn_concat_inference(input_tensor,filter_height_list,filter_width,\\\n",
    "                         in_channels,out_channels,layer_name_list,keep_prob):\n",
    "    conv_pool_list = []\n",
    "    filter_num = len(filter_height_list)\n",
    "    for i in range(filter_num):\n",
    "        conv_pool = conv_and_pooling(input_tensor,filter_height_list[i],filter_width,\\\n",
    "                                     in_channels,out_channels,layer_name_list[i]) \n",
    "        conv_pool_list.append(conv_pool)\n",
    "\n",
    "    # concatenant all the conv_pools tensor\n",
    "    conv_pool_concat = tf.concat([conv_pool for conv_pool in conv_pool_list],1)\n",
    "\n",
    "    output = fc_output_inference(conv_pool_concat,FC_SIZE,NUM_CLASSES,keep_prob)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_conv3_output(input_X,EMBEDDING_SIZE,keep_prob):\n",
    "    return cnn_mono_inference(input_X,3,EMBEDDING_SIZE,\\\n",
    "                                           1,128,\"Conv3-128\",keep_prob)\n",
    "\n",
    "def model_conv4_output(input_X,EMBEDDING_SIZE,keep_prob):\n",
    "    return cnn_mono_inference(input_X,4,EMBEDDING_SIZE,\\\n",
    "                                           1,128,\"Conv4-128\",keep_prob)\n",
    "\n",
    "def model_conv5_output(input_X,EMBEDDING_SIZE,keep_prob):\n",
    "    return cnn_mono_inference(input_X,5,EMBEDDING_SIZE,\\\n",
    "                                           1,128,\"Conv5-128\",keep_prob)\n",
    "\n",
    "\n",
    "def model_conv6_output(input_X,EMBEDDING_SIZE,keep_prob):\n",
    "    return cnn_mono_inference(input_X,6,EMBEDDING_SIZE,\\\n",
    "                                           1,128,\"Conv6-128\",keep_prob)\n",
    "def model_conv7_output(input_X,EMBEDDING_SIZE,keep_prob):\n",
    "    return cnn_mono_inference(input_X,7,EMBEDDING_SIZE,\\\n",
    "                                           1,128,\"Conv7-128\",keep_prob)\n",
    "\n",
    "def model_concat_output(input_X,EMBEDDING_SIZE,keep_prob):\n",
    "    concat_output = cnn_concat_inference\\\n",
    "                         (input_X,[3,4,5,6],EMBEDDING_SIZE,1,32,\\\n",
    "                          [\"Conv3-32\",\"Conv4-32\",\"Conv5-32\",\"Conv6-32\"],keep_prob)\n",
    "def model_concat2_output(input_X,EMBEDDING_SIZE,keep_prob):\n",
    "    concat_output = cnn_concat_inference\\\n",
    "                         (input_X,[3,4,5,6,7],EMBEDDING_SIZE,1,32,\\\n",
    "                          [\"Conv3-32\",\"Conv4-32\",\"Conv5-32\",\"Conv6-32\",\"Conv7-32\"],keep_prob)\n",
    "    return concat_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0915 15:46:25.964170 139925806647104 deprecation.py:506] From <ipython-input-29-bf2d9408005b>:12: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model Conv3-128 was constructed!\n",
      "model Conv-concat was constructed!\n"
     ]
    }
   ],
   "source": [
    "cnn_mono_inference(input_X,3,EMBEDDING_SIZE,1,128,\"Conv3-128\",keep_prob)\n",
    "print (\"model Conv3-128 was constructed!\")\n",
    "cnn_concat_inference(input_X,[3,4,5,6],EMBEDDING_SIZE,1,32,\\\n",
    "                         [\"Conv3-32\",\"Conv4-32\",\"Conv5-32\",\"Conv6-32\"],keep_prob)\n",
    "print (\"model Conv-concat was constructed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_15:0' shape=(?, 2) dtype=float32>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv3_output(input_X,EMBEDDING_SIZE,keep_prob)\n",
    "model_conv4_output(input_X,EMBEDDING_SIZE,keep_prob)\n",
    "model_conv5_output(input_X,EMBEDDING_SIZE,keep_prob)\n",
    "model_conv6_output(input_X,EMBEDDING_SIZE,keep_prob)\n",
    "model_concat_output(input_X,EMBEDDING_SIZE,keep_prob)\n",
    "model_concat2_output(input_X,EMBEDDING_SIZE,keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_op(predic_output,input_ys):\n",
    "    #calculate TP，TN，FP，FN on test_data\n",
    "    predictions = tf.argmax(predic_output, 1)\n",
    "    actuals = tf.argmax(input_ys, 1)\n",
    "\n",
    "    ones_like_actuals = tf.ones_like(actuals)\n",
    "    zeros_like_actuals = tf.zeros_like(actuals)\n",
    "    ones_like_predictions = tf.ones_like(predictions)\n",
    "    zeros_like_predictions = tf.zeros_like(predictions)\n",
    "\n",
    "    tn_op = tf.reduce_sum(\\\n",
    "        tf.cast(\\\n",
    "            tf.logical_and(\\\n",
    "            tf.equal(actuals, ones_like_actuals),\\\n",
    "            tf.equal(predictions, ones_like_predictions)\\\n",
    "            ), \\\n",
    "            \"float\")\\\n",
    "        )\n",
    "\n",
    "    tp_op = tf.reduce_sum(\\\n",
    "        tf.cast(\\\n",
    "            tf.logical_and(\\\n",
    "            tf.equal(actuals, zeros_like_actuals),\\\n",
    "            tf.equal(predictions, zeros_like_predictions)\\\n",
    "            ),\\\n",
    "            \"float\")\\\n",
    "        )\n",
    "\n",
    "    fn_op = tf.reduce_sum(\\\n",
    "        tf.cast(\\\n",
    "          tf.logical_and(\\\n",
    "            tf.equal(actuals, zeros_like_actuals),\\\n",
    "            tf.equal(predictions, ones_like_predictions)\\\n",
    "          ),\\\n",
    "          \"float\")\\\n",
    "        )\n",
    "\n",
    "    fp_op = tf.reduce_sum(\\\n",
    "        tf.cast(\\\n",
    "          tf.logical_and(\\\n",
    "            tf.equal(actuals, ones_like_actuals),\\\n",
    "            tf.equal(predictions, zeros_like_predictions)\\\n",
    "          ),\\\n",
    "          \"float\")\\\n",
    "        )\n",
    "    \n",
    "    return predictions,actuals,tp_op, tn_op,fp_op, fn_op\n",
    "\n",
    "\n",
    "\n",
    "def print_test_evaluation(tp,tn,fp,fn):  \n",
    "    tpr = float(tp)/(float(tp) + float(fn))\n",
    "    recall = tpr\n",
    "    print(\"Sensitivity/recall on the test data is :{}\".format(tpr)) \n",
    "\n",
    "    specifity = float(tn)/(float(tn) + float(fp))\n",
    "    print(\"specifity on the test data is :{}\".format(specifity)) \n",
    "\n",
    "    precision = float(tp)/(float(tp) + float(fp))\n",
    "    print(\"precision on the test data is :{}\".format(precision))\n",
    "\n",
    "    f1_score = (2 * (precision * recall)) / (precision + recall)\n",
    "    print(\"f1_score on the test data is :{}\".format(f1_score))\n",
    "\n",
    "    fpr = float(fp)/(float(tp) + float(fn))\n",
    "    print(\"fpr on the test data is :{}\".format(fpr))\n",
    "    if tp!=0 and fp!=0 and tn!=0 and fn!=0:\n",
    "        mcc = ((float(tp) * float(tn)) - (float(fp) * float(fn))) /\\\n",
    "                math.sqrt((float(tp) + float(fp)) * (float(tp) + float(fn))*\\\n",
    "                 (float(tn) + float(fp)) * (float(tn) + float(fn)))\n",
    "        print(\"mcc on the test data is :{}\".format(mcc))\n",
    "    \n",
    "    \n",
    "    accuracy = (float(tp) + float(tn))/(float(tp) + float(fp) + float(fn) + float(tn))\n",
    "    if accuracy > 0.92:\n",
    "        print(\"dddddDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD\")\n",
    "    print(\"accuracy on the test data is :{}\".format(accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(786, 2)\n",
      "(786, 164, 4, 1)\n",
      "(338, 2)\n",
      "(338, 164, 4, 1)\n",
      "dataset vectorization finished!\n",
      "iteration 10000\n"
     ]
    }
   ],
   "source": [
    "LR = 0.0001      #learning rate\n",
    "TRAINING_ITER = 10000   #iteration times\n",
    "BATCH_SIZE = 18        #batch size of input\n",
    " \n",
    "SEQUENCE_LENGTH = 164   #sequence length of input\n",
    "EMBEDDING_SIZE = 4      #char embedding size(sequence width of input)\n",
    "# CONV_SIZE = 3    #first filter size\n",
    " # CONV_DEEP = 128   #number of first filter(convolution deepth)\n",
    "  \n",
    "STRIDES = [1,1,1,1]  #the strid in each of four dimensions during convolution\n",
    "KSIZE = [1,164,1,1]    #pooling window size\n",
    "\n",
    "FC_SIZE = 1024     #nodes of full-connection layer\n",
    "NUM_CLASSES = 2   # classification number\n",
    " \n",
    "DROPOUT_KEEP_PROB = 0.40   #keep probability of dropout\n",
    "\n",
    "\n",
    "\n",
    "FILE_PATH = \"/home/emon/Downloads/cnnMirtronPred-master/data/miRBase_set.csv\"\n",
    "FILE_PATH_PUTATIVE = \"/home/emon/Downloads/cnnMirtronPred-master/data/putative_mirtrons_set.csv\"\n",
    "all_data_array = read_data(FILE_PATH,FILE_PATH_PUTATIVE)\n",
    "\n",
    "x,y= vectorize_data(all_data_array)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.30, random_state=42)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "print(y_train.shape)\n",
    "print(X_train.shape)\n",
    "print(y_test.shape)\n",
    "print(X_test.shape)\n",
    "print(\"dataset vectorization finished!\")\n",
    "print(\"iteration\",TRAINING_ITER)\n",
    "dataset_size = len(X_train)  #number of training dataset\n",
    "\n",
    "input_X = tf.placeholder(tf.float32,[None,SEQUENCE_LENGTH,EMBEDDING_SIZE,1])\n",
    "input_y = tf.placeholder(tf.float32,[None, NUM_CLASSES])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "def train_model_concat2(X_train=X_train,y_train=y_train,X_test=X_test,y_test=y_test,LR=LR,DROPOUT_KEEP_PROB=DROPOUT_KEEP_PROB,dataset_size=dataset_size):\n",
    "    concat_output = model_concat2_output(input_X,EMBEDDING_SIZE,keep_prob)\n",
    "    \n",
    "    #loss_and_optimization(conv6_output,y_train)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = concat_output,labels = input_y)\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    # optimization\n",
    "    train_op = tf.train.RMSPropOptimizer(LR).minimize(cross_entropy_mean)  \n",
    "    \n",
    "    # calculate the accuracy of the model\n",
    "    correct_prediction = tf.equal(tf.argmax(concat_output,1), tf.argmax(input_y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,\"float\"))\n",
    "    \n",
    "    a = tf.placeholder(tf.float32,[None, NUM_CLASSES])\n",
    "    b = tf.placeholder(tf.float32,[None, NUM_CLASSES])\n",
    "    auc = tf.metrics.auc(a, b)\n",
    "    \n",
    "    tf.summary.scalar(\"loss\",cross_entropy_mean)\n",
    "    tf.summary.scalar(\"accuracy\",accuracy)\n",
    "\n",
    "    # evaluation operation test_data\n",
    "    prd,actu,tp_op,tn_op,fp_op,fn_op = evaluation_op(concat_output,input_y)\n",
    " \n",
    "    merged = tf.summary.merge_all()\n",
    "    #saver = tf.train.Saver()\n",
    "    #itr=[]\n",
    "    #crs=[]\n",
    "    # train the model with the training dataset\n",
    "    with tf.Session() as sess:  # run the session        \n",
    "        #writer = tf.summary.FileWriter(log_path, sess.graph)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.initialize_local_variables())\n",
    "        for i in range(TRAINING_ITER): \n",
    "            start = (i * BATCH_SIZE)% dataset_size\n",
    "            end = min(start + BATCH_SIZE,dataset_size)\n",
    "            batch_xs = X_train[start:end]\n",
    "            batch_ys = y_train[start:end]\n",
    "            #sess.run(train_op,feed_dict={input_X:batch_xs,input_y:batch_ys,\\\n",
    "            #                              keep_prob:DROPOUT_KEEP_PROB})\n",
    "            \n",
    "            _,rs = sess.run([train_op,cross_entropy_mean],\\\n",
    "                            feed_dict={input_X:batch_xs,input_y:batch_ys,\\\n",
    "                                      keep_prob:DROPOUT_KEEP_PROB})\n",
    "            #writer.add_summary(rs,i)\n",
    "            \n",
    "          #  print loss and accuracy during the training process\n",
    "            #itr.append(i)\n",
    "            #crs.append(sess.run(cross_entropy_mean,feed_dict={input_X:batch_xs,input_y:batch_ys,keep_prob:DROPOUT_KEEP_PROB}))\n",
    "            if(i%1000==0):\n",
    "                print(\"The {} iteration:\".format(i))\n",
    "                print(\"The cross_entropy_mean为：\",end='')\n",
    "                print(sess.run(cross_entropy_mean,\\\n",
    "                               feed_dict={input_X:batch_xs,input_y:batch_ys,\\\n",
    "                                          keep_prob:DROPOUT_KEEP_PROB}))\n",
    "                #print(sess.run(cross_entropy_mean,\\\n",
    "                #               feed_dict={input_X:batch_xs,input_y:batch_ys}))\n",
    "              #  print(\"The accuracy on batch data:\",end='')\n",
    "              #  print(sess.run(accuracy,feed_dict={input_xs:batch_xs,\\\n",
    "              #                  input_ys:batch_ys,keep_prob:1}))\n",
    "                print(\"The accuracy on training data:\",end='')\n",
    "                print(sess.run(accuracy,feed_dict={input_X:X_train,\\\n",
    "                               input_y:y_train,keep_prob:1}))\n",
    "               # print(sess.run(accuracy,feed_dict={input_X:X_train,\\\n",
    "                #               input_y:y_train}))\n",
    "                print(\"The accuracy on test data:\",end='')\n",
    "                print(sess.run(accuracy,feed_dict={input_X:X_test,\\\n",
    "                                           input_y:y_test,keep_prob:1}))\n",
    "                print(\"==================\")\n",
    "                \n",
    "            #saver.save(sess,model_path)  \n",
    "        #print(tf.metrics.auc(y_test,correct_prediction))      \n",
    "        print(\"*********training finished********\")\n",
    "        print(\"performance on the test dataset:\")\n",
    "        pr,act,tp,tn,fp,fn = sess.run([prd,actu,tp_op, tn_op,fp_op, fn_op],\\\n",
    "                                feed_dict={input_X:X_test,\\\n",
    "                                input_y:y_test,keep_prob:1})\n",
    "        print(\"tp:{},tn:{},fp:{},fn:{}\".format(tp,tn,fp,fn))\n",
    "        #train_auc = sess.run(auc,feed_dict={a:y_test,b:concat_output})\n",
    "        acc=print_test_evaluation(tp,tn,fp,fn)\n",
    "        return pr,act,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search \n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "LR = np.arange(0.0001, 0.001, 0.0001) #Learning rate range from 0.0001 to 0.001\n",
    "TRAINING_ITER = 10000   #iteration times\n",
    "BATCH_SIZE = 18        #batch size of input\n",
    " \n",
    "SEQUENCE_LENGTH = 164   #sequence length of input\n",
    "EMBEDDING_SIZE = 4      #char embedding size(sequence width of input)\n",
    "# CONV_SIZE = 3    #first filter size\n",
    " # CONV_DEEP = 128   #number of first filter(convolution deepth)\n",
    "  \n",
    "STRIDES = [1,1,1,1]  #the strid in each of four dimensions during convolution\n",
    "KSIZE = [1,164,1,1]    #pooling window size\n",
    "\n",
    "FC_SIZE = 1024     #nodes of full-connection layer\n",
    "NUM_CLASSES = 2   # classification number\n",
    " \n",
    "DROPOUT_KEEP_PROB = np.arange(0.40, 0.64, 0.04)   #keep probability of dropout\n",
    "\n",
    "\n",
    "FILE_PATH = \"miRBase_set.csv\"\n",
    "FILE_PATH_PUTATIVE = \"putative_mirtrons_set.csv\"\n",
    "all_data_array = read_data(FILE_PATH,FILE_PATH_PUTATIVE)\n",
    "x,y,vectorized_dataset = vectorize_data(all_data_array)\n",
    "#X_train, y_train, X_test, y_test = data_partition(vectorized_dataset)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.30, random_state=42)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test =np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "print(y_train.shape)\n",
    "print(X_train.shape)\n",
    "print(y_test.shape)\n",
    "print(X_test.shape)\n",
    "print(\"dataset vectorization finished!\")\n",
    "print(\"iteration\",TRAINING_ITER)\n",
    "dataset_size = len(X_train)  #number of training dataset\n",
    "\n",
    "input_X = tf.placeholder(tf.float32,[None,SEQUENCE_LENGTH,EMBEDDING_SIZE,1])\n",
    "input_y = tf.placeholder(tf.float32,[None, NUM_CLASSES])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "kfold = KFold(5, True, 1)\n",
    "\n",
    "# enumerate splits\n",
    "m=0\n",
    "a1=a2=0\n",
    "lrs=[]\n",
    "drs=[]\n",
    "acs=[]\n",
    "for l in LR:\n",
    "    for d in DROPOUT_KEEP_PROB:\n",
    "        print(\"Learning Rate: \",l)\n",
    "        print(\"Dropout: \",d,\"\\n\\n\")\n",
    "        lrs.append(d)\n",
    "        drs.append(l)\n",
    "        s=0\n",
    "        for train, test in kfold.split(X_train):\n",
    "            xtr, xts = X_train[train], X_train[test]\n",
    "            ytr, yts = y_train[train], y_train[test]\n",
    "            dataset_size = len(xtr)\n",
    "            print(xtr.shape)\n",
    "            print(xts.shape)\n",
    "            print(ytr.shape)\n",
    "            print(yts.shape)\n",
    "            _,_,a=train_model_concat2(xtr,ytr,xts,yts,l,d,dataset_size)\n",
    "            s=s+float(a)\n",
    "        ac=s/5\n",
    "        acs.append(ac)\n",
    "        print(\"AAAAAAAAAAA: \",ac)\n",
    "        print(\"\\n\\n\")\n",
    "        if m<ac:\n",
    "            m=ac\n",
    "            a1=l\n",
    "            a2=d\n",
    "print(a1)\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr,au,ac=train_model_concat2(X_train,y_train,X_test,y_test,0.0001,0.40,len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting ROC curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "FPR, TPR, _ = roc_curve(ac6, pr6)\n",
    "AUC = auc(FPR, TPR)\n",
    "#f1,t1,_= roc_curve(act2, pr2)\n",
    "#AUC2 = auc(f1, t1)\n",
    "# calculate roc curve\n",
    "plt.figure()\n",
    "plt.plot(FPR,TPR,label='ROC curve (area = %0.3f)' % AUC)\n",
    "#plt.plot(f1,t1,label='ROC curve concat(area = %0.3f)' % AUC2)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.02])\n",
    "plt.grid(True,'both')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('image.jpg')\n",
    "plt.show()\n",
    "#plt.savefig('testplot.png')\n",
    "#Image.open('testplot.png').save('testplot.jpg','JPEG')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
